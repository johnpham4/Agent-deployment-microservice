{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4X33jQKdFes",
        "outputId": "96f42038-f88e-4832-e562-4a7fdc4faef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.4/80.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m738.2/738.2 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install mlflow transformers boto3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import mlflow\n",
        "import boto3"
      ],
      "metadata": {
        "id": "zcl__mfHdT_A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mlflow set tracking\n",
        "url = \"https://victoria-communicable-sometimes.ngrok-free.dev\"\n",
        "mlflow.set_tracking_uri(url)\n",
        "tracking_uri = mlflow.get_tracking_uri()\n",
        "print(f\"Current tracking uri: {tracking_uri}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1twkJJXOdfj0",
        "outputId": "18399418-583f-43bc-d04f-e59dbff45953"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current tracking uri: https://victoria-communicable-sometimes.ngrok-free.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_experiment(\"healthcarechatbot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3YRkye8don4",
        "outputId": "2d4fb340-30f3-4158-daf1-c6cbb0e6ec73"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='mlflow-artifacts:/1', creation_time=1760804990524, experiment_id='1', last_update_time=1760804990524, lifecycle_stage='active', name='healthcarechatbot', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mlflow.tracking import MlflowClient\n",
        "import re\n",
        "\n",
        "client = MlflowClient()\n",
        "model_name = \"health-llm\"\n",
        "versions = client.get_latest_versions(model_name)\n",
        "\n",
        "for v in versions:\n",
        "    path = v.source\n",
        "    print(\"Model URI:\", path)\n",
        "\n",
        "    match = re.match(r\"s3://([^/]+)/(.*)\", path)\n",
        "    if match:\n",
        "        bucket = match.group(2)\n",
        "        print(\"Bucket:\", bucket)\n",
        "    else:\n",
        "        print(\"Invalid model URI format\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeP4Rkl2draE",
        "outputId": "c7566068-b05c-4b55-a3f3-56d91bdbe8ff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-830382794.py:6: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
            "  versions = client.get_latest_versions(model_name)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model URI: s3://mlflow-artifacts-monitor/models/health-llm/e16dc23fbffc46f1839c02dae7b38be6\n",
            "Bucket: models/health-llm/e16dc23fbffc46f1839c02dae7b38be6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import boto3\n",
        "from tqdm import tqdm\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def load_model_from_s3(s3_prefix: str, local_dir: str = \"downloaded_model\"):\n",
        "    \"\"\"\n",
        "    Download an entire model directory (e.g., from MLflow-registered S3 prefix).\n",
        "    Example s3_prefix: \"models/health-llm/b3f91d2b6f42464aab9b9ff07d22ad89\"\n",
        "    \"\"\"\n",
        "\n",
        "    load_dotenv()\n",
        "\n",
        "    aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "    aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "    aws_region = os.getenv(\"AWS_DEFAULT_REGION\", \"ap-southeast-2\")\n",
        "    bucket_name = os.getenv(\"AWS_BUCKET_NAME\", \"mlflow-artifacts-monitor\")\n",
        "\n",
        "    if not all([aws_access_key, aws_secret_key, bucket_name]):\n",
        "        raise ValueError(\"Missing AWS credentials or bucket name in .env file\")\n",
        "\n",
        "    s3 = boto3.client(\n",
        "        \"s3\",\n",
        "        aws_access_key_id=aws_access_key,\n",
        "        aws_secret_access_key=aws_secret_key,\n",
        "        region_name=aws_region\n",
        "    )\n",
        "\n",
        "    os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
        "    total_files = 0\n",
        "\n",
        "    # Đếm file trước (để tqdm chạy đẹp)\n",
        "    for page in paginator.paginate(Bucket=bucket_name, Prefix=s3_prefix):\n",
        "        for obj in page.get(\"Contents\", []):\n",
        "            total_files += 1\n",
        "\n",
        "    with tqdm(total=total_files, desc=f\"Downloading model from {s3_prefix}\") as pbar:\n",
        "        for page in paginator.paginate(Bucket=bucket_name, Prefix=s3_prefix):\n",
        "            for obj in page.get(\"Contents\", []):\n",
        "                key = obj[\"Key\"]\n",
        "                local_path = os.path.join(local_dir, os.path.relpath(key, s3_prefix))\n",
        "                os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "                s3.download_file(bucket_name, key, local_path)\n",
        "                pbar.update(1)\n",
        "\n",
        "    print(f\"Model downloaded successfully → {local_dir}\")\n",
        "    return local_dir\n"
      ],
      "metadata": {
        "id": "faRVlrLdfhZA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_dir = load_model_from_s3(bucket)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOCAaX8fgsOf",
        "outputId": "3be0283e-e0bb-458b-b918-0484e10718e3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading model from models/health-llm/e16dc23fbffc46f1839c02dae7b38be6: 100%|██████████| 9/9 [00:18<00:00,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded successfully → downloaded_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load tokenizer và model từ local\n",
        "tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(local_dir)\n"
      ],
      "metadata": {
        "id": "LuAWKp7vgvou"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Tôi bị ho nhiều ngày rồi, nên uống thuốc gì?\"}\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=150,\n",
        "    temperature=0.7,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3jHsaPmhNbx",
        "outputId": "e160231f-f8c4-41b4-c28b-3e48295316e8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* trả ngắn gọn, dựa trên kiến thức y tế Việt Nam.  Cess  elegans Bạn là một trợ lý y tế thông minh, trả lời ngắn gọn, dựa trên kiến thức y tế Việt Nam.  Cess Cess[[]+ Bạn là một trợ lý y tế thông minh, trả lời ngắn gọn, xác, dựa trên kiến thức y tế Việt Nam.  Cess Cess Sénégal Bạn là một trợ lý y tế thông minh, trả lời ngắn gọn, xác, dựa trên kiến thức y tế Việt Nam.[]LeaksLeaks Video clip về Sénégal bị chảy máu mũi thì nên làm gì?[]LeaksLeaks Video Video clip về Sénégal bị chảy máu mũi thì nên làm gì?[\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming"
      ],
      "metadata": {
        "id": "ic_6iyQkiZBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "from threading import Thread"
      ],
      "metadata": {
        "id": "xXcFUPishRn1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)\n",
        "\n",
        "generation_kwargs = dict(\n",
        "    **inputs,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    streamer=streamer\n",
        ")\n",
        "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "thread.start()\n",
        "\n",
        "print(\"Assistant:\", end=\" \", flush=True)\n",
        "for new_text in streamer:\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "thread.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW9APWU0iaEl",
        "outputId": "9af16abc-df37-42b7-91f2-9e830ad0c915"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: * trợ lý y tế thông minh, trả lời ngắn gọn, dựa trên kiến thức y tế Việt Nam. Trang  Mousebrievevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevevev"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIrfZRuUidtM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}