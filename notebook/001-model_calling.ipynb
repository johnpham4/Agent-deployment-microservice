{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4X33jQKdFes",
        "outputId": "3f24d59b-c5d3-4aec-dfde-0e59eaf24bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install mlflow transformers boto3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import mlflow\n",
        "import boto3"
      ],
      "metadata": {
        "id": "zcl__mfHdT_A"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mlflow set tracking\n",
        "url = \"https://victoria-communicable-sometimes.ngrok-free.dev\"\n",
        "mlflow.set_tracking_uri(url)\n",
        "tracking_uri = mlflow.get_tracking_uri()\n",
        "print(f\"Current tracking uri: {tracking_uri}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1twkJJXOdfj0",
        "outputId": "d0cb8ded-422a-46b1-a1b8-79d3ff17d135"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current tracking uri: https://victoria-communicable-sometimes.ngrok-free.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_experiment(\"healthcarechatbot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3YRkye8don4",
        "outputId": "c0a981a2-8345-4b2f-8e93-ecf226ec5f1d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='mlflow-artifacts:/1', creation_time=1760322730708, experiment_id='1', last_update_time=1760322730708, lifecycle_stage='active', name='healthcarechatbot', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "client = MlflowClient()\n",
        "model_name = \"health-llm\"\n",
        "\n",
        "# Lấy thông tin model versions\n",
        "versions = client.get_latest_versions(model_name)\n",
        "\n",
        "for v in versions:\n",
        "    print(\"Version:\", v.version)\n",
        "    path = v.source\n",
        "    print(\"Model URI:\", path)\n",
        "    print(\"Run ID:\", v.run_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeP4Rkl2draE",
        "outputId": "8e0be1d5-2d51-4005-ff6e-f75120a8bc48"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-272760428.py:7: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
            "  versions = client.get_latest_versions(model_name)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version: 1\n",
            "Model URI: s3://mlflow-artifacts-monitor/models/health-llm/a067fa2b0e724057a797ead349550265\n",
            "Run ID: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import boto3\n",
        "from tqdm import tqdm\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def load_model_from_s3(s3_prefix: str, local_dir: str = \"downloaded_model\"):\n",
        "    \"\"\"\n",
        "    Download an entire model directory (e.g., from MLflow-registered S3 prefix).\n",
        "    Example s3_prefix: \"models/health-llm/b3f91d2b6f42464aab9b9ff07d22ad89\"\n",
        "    \"\"\"\n",
        "\n",
        "    load_dotenv()\n",
        "\n",
        "    aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "    aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "    aws_region = os.getenv(\"AWS_DEFAULT_REGION\", \"ap-southeast-2\")\n",
        "    bucket_name = os.getenv(\"AWS_BUCKET_NAME\", \"mlflow-artifacts-monitor\")\n",
        "\n",
        "    if not all([aws_access_key, aws_secret_key, bucket_name]):\n",
        "        raise ValueError(\"Missing AWS credentials or bucket name in .env file\")\n",
        "\n",
        "    s3 = boto3.client(\n",
        "        \"s3\",\n",
        "        aws_access_key_id=aws_access_key,\n",
        "        aws_secret_access_key=aws_secret_key,\n",
        "        region_name=aws_region\n",
        "    )\n",
        "\n",
        "    os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
        "    total_files = 0\n",
        "\n",
        "    # Đếm file trước (để tqdm chạy đẹp)\n",
        "    for page in paginator.paginate(Bucket=bucket_name, Prefix=s3_prefix):\n",
        "        for obj in page.get(\"Contents\", []):\n",
        "            total_files += 1\n",
        "\n",
        "    with tqdm(total=total_files, desc=f\"Downloading model from {s3_prefix}\") as pbar:\n",
        "        for page in paginator.paginate(Bucket=bucket_name, Prefix=s3_prefix):\n",
        "            for obj in page.get(\"Contents\", []):\n",
        "                key = obj[\"Key\"]\n",
        "                local_path = os.path.join(local_dir, os.path.relpath(key, s3_prefix))\n",
        "                os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "                s3.download_file(bucket_name, key, local_path)\n",
        "                pbar.update(1)\n",
        "\n",
        "    print(f\"Model downloaded successfully → {local_dir}\")\n",
        "    return local_dir\n"
      ],
      "metadata": {
        "id": "faRVlrLdfhZA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_dir = load_model_from_s3(\"models/health-llm/a067fa2b0e724057a797ead349550265\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOCAaX8fgsOf",
        "outputId": "547127b9-0319-4c23-8582-ec2bbba77285"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading model from models/health-llm/a067fa2b0e724057a797ead349550265: 100%|██████████| 9/9 [00:19<00:00,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded successfully → downloaded_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_dir = \"downloaded_model\"\n",
        "\n",
        "# Load tokenizer và model từ local\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n"
      ],
      "metadata": {
        "id": "LuAWKp7vgvou"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_dir = \"downloaded_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
        "\n",
        "# Chat history\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Bạn là một trợ lý y tế thông minh, trả lời ngắn gọn, chính xác, dựa trên kiến thức y tế Việt Nam.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Con tôi bị chảy máu mũi thì nên làm gì?\"}\n",
        "]\n",
        "\n",
        "# Dùng built-in chat template\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,   # chỉ tạo text string, không tokenize luôn\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=80,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3jHsaPmhNbx",
        "outputId": "40b30308-9db1-4cf0-f774-aa1983ed06aa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Tôi bị chảy máu mũi+, _Z, []]!> j thực tế.& W vàỴ y tế Việt Nam. Ẽ Tôi bị chảy máu mũiỠ tôi là một trợ lý y tế thông minh.Ẫ lý y tế thông minh.Ẳ!Ằ tư vấn viênÕ Ẻ Tôi là một\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming"
      ],
      "metadata": {
        "id": "ic_6iyQkiZBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer"
      ],
      "metadata": {
        "id": "xXcFUPishRn1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import Thread\n",
        "# --- Tạo streamer ---\n",
        "streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)\n",
        "\n",
        "# --- Chạy generate trong thread riêng để không block ---\n",
        "generation_kwargs = dict(\n",
        "    **inputs,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    streamer=streamer\n",
        ")\n",
        "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
        "thread.start()\n",
        "\n",
        "# --- Stream output dần ---\n",
        "print(\"Assistant:\", end=\" \", flush=True)\n",
        "for new_text in streamer:\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "thread.join()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW9APWU0iaEl",
        "outputId": "593eadf1-2b56-405d-95d0-db33fbb01497"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: *+_Z  Tôi bị chảy máu mũi[]]! > j và phương pháp Việt Nam. & W ỴẼ, vàỠẪẲ có thểẰ làÕ Ẻ Ặ.Ỗ, thích hợp cho nhiều người Việt Nam.      Ẹ Ỹ tôi bị chảy máu mũi thì nên làm gì?          È.Ữ chảy máu mũiỰỢ  Ử tôi bị chảy máu mũi thì nên làm gì? Ể Tôi bị chảy máu mũi thì nên làm gì"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIrfZRuUidtM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}