# QA Chatbot API Documentation

## ğŸš€ CÃ¡ch cháº¡y API

### Sá»­ dá»¥ng script (khuyáº¿n nghá»‹):

```bash
# Windows
start_server.bat

# Linux/Mac
chmod +x start_server.sh
./start_server.sh
```

### Cháº¡y trá»±c tiáº¿p:

```bash
cd src/app
python main.py
```

API sáº½ cháº¡y táº¡i: **http://localhost:8000**

## ğŸ“‹ API Endpoints

### 1. Health Check

```http
GET /api/v1/chat/health
```

**Response:**

```json
{
  "status": "healthy",
  "service": "chat-service",
  "version": "1.0.0",
  "model_loaded": true,
  "memory_usage_mb": 1024.5,
  "timestamp": 1694123456.789
}
```

### 2. Chat Generation (Main endpoint)

```http
POST /api/v1/chat/generate
```

**Request Body:**

```json
{
  "message": "What is the SQL query to get all users?",
  "session_id": "optional-session-id",
  "max_tokens": 150,
  "temperature": 0.7
}
```

**Response:**

```json
{
  "response": "SELECT * FROM users;",
  "session_id": "abc123-def456-789",
  "timestamp": 1694123456.789,
  "response_time": 1.25,
  "model_used": "custom-llama"
}
```

### 3. Model Management

#### Load Model

```http
POST /api/v1/chat/model/load
```

**Optional Body:**

```json
{
  "model_path": "/custom/path/to/model"
}
```

#### Model Status

```http
GET /api/v1/chat/model/status
```

**Response:**

```json
{
  "model_loaded": true,
  "model_path": "/app/merged_model",
  "device": "cuda",
  "timestamp": 1694123456.789
}
```

### 4. Chat History

```http
GET /api/v1/chat/history/{session_id}?limit=50
```

### 5. Analytics

```http
GET /api/v1/chat/analytics?days=7
```

## ğŸ§ª Test API

### Sá»­ dá»¥ng test script:

```bash
python test_api.py
```

### Sá»­ dá»¥ng curl:

```bash
# Health check
curl http://localhost:8000/api/v1/chat/health

# Chat generation
curl -X POST http://localhost:8000/api/v1/chat/generate \
  -H "Content-Type: application/json" \
  -d '{
    "message": "SELECT all users from database",
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

### Sá»­ dá»¥ng Python requests:

```python
import requests

# Chat vá»›i API
response = requests.post(
    "http://localhost:8000/api/v1/chat/generate",
    json={
        "message": "How to create a table in SQL?",
        "max_tokens": 150,
        "temperature": 0.7
    }
)

result = response.json()
print(f"AI Response: {result['response']}")
```

## ğŸ“Š API Documentation

Khi server cháº¡y, truy cáº­p:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## âš™ï¸ Configuration

API sá»­ dá»¥ng config tá»« `core/config.py`:

```python
# Model settings
MODEL_PATH = "/app/merged_model"  # ÄÆ°á»ng dáº«n model
MAX_TOKENS = 200                 # Max tokens máº·c Ä‘á»‹nh
TEMPERATURE = 0.7                # Temperature máº·c Ä‘á»‹nh

# Chat template
CHAT_TEMPLATE = "..."            # Template cho chat
```

## ğŸ”§ Troubleshooting

### Lá»—i thÆ°á»ng gáº·p:

1. **Model not loaded**

   ```
   HTTP 503: AI model is not available
   ```

   - Kiá»ƒm tra MODEL_PATH cÃ³ Ä‘Ãºng khÃ´ng
   - Gá»i endpoint `/api/v1/chat/model/load` Ä‘á»ƒ load model

2. **Out of memory**

   ```
   CUDA out of memory
   ```

   - Giáº£m max_tokens
   - Sá»­ dá»¥ng CPU thay vÃ¬ GPU

3. **Import errors**
   ```
   ModuleNotFoundError: No module named 'xxx'
   ```
   - CÃ i Ä‘áº·t dependencies: `pip install -r requirements.txt`

### Debug:

1. Kiá»ƒm tra logs trong terminal
2. Test tá»«ng endpoint riÃªng biá»‡t
3. Kiá»ƒm tra model files cÃ³ tá»“n táº¡i khÃ´ng

## ğŸš€ Deployment

### Docker:

```bash
docker build -t qa-chatbot .
docker run -p 8000:8000 qa-chatbot
```

### Production:

```bash
# Sá»­ dá»¥ng Gunicorn
pip install gunicorn
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
```

## ğŸ“ˆ Monitoring

- Memory usage: `/api/v1/chat/health`
- Response times: Included in chat responses
- Analytics: `/api/v1/chat/analytics`

## ğŸ” Security Notes

- Trong production, cáº¥u hÃ¬nh CORS origins cá»¥ thá»ƒ
- ThÃªm authentication náº¿u cáº§n
- Rate limiting Ä‘Ã£ Ä‘Æ°á»£c implement trong middleware
- Validate input Ä‘á»ƒ trÃ¡nh injection attacks
